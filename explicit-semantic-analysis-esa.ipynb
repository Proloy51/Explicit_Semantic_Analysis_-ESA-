{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:43:40.008448Z","iopub.execute_input":"2026-01-08T11:43:40.008810Z","iopub.status.idle":"2026-01-08T11:43:40.014802Z","shell.execute_reply.started":"2026-01-08T11:43:40.008778Z","shell.execute_reply":"2026-01-08T11:43:40.013906Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"**Import Libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:43:40.016429Z","iopub.execute_input":"2026-01-08T11:43:40.016688Z","iopub.status.idle":"2026-01-08T11:43:40.031106Z","shell.execute_reply.started":"2026-01-08T11:43:40.016664Z","shell.execute_reply":"2026-01-08T11:43:40.030195Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"**Step 1: Define the Dataset**","metadata":{}},{"cell_type":"code","source":"documents = [\n    \"scary green crocodile\",\n    \"scary green big\",\n    \"small crocodile\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:43:40.032326Z","iopub.execute_input":"2026-01-08T11:43:40.033151Z","iopub.status.idle":"2026-01-08T11:43:40.046586Z","shell.execute_reply.started":"2026-01-08T11:43:40.033119Z","shell.execute_reply":"2026-01-08T11:43:40.045814Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"**Step 2: Compute TF-IDF Matrix**","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\n\ntfidf_matrix = vectorizer.fit_transform(documents)\n\nprint(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:43:40.048658Z","iopub.execute_input":"2026-01-08T11:43:40.049042Z","iopub.status.idle":"2026-01-08T11:43:40.065454Z","shell.execute_reply.started":"2026-01-08T11:43:40.049016Z","shell.execute_reply":"2026-01-08T11:43:40.064573Z"}},"outputs":[{"name":"stdout","text":"TF-IDF Matrix:\n [[0.         0.57735027 0.57735027 0.57735027 0.        ]\n [0.68091856 0.         0.51785612 0.51785612 0.        ]\n [0.         0.60534851 0.         0.         0.79596054]]\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"**Step 3: Visualize the Terms and Their Corresponding Index**","metadata":{}},{"cell_type":"code","source":"terms = vectorizer.get_feature_names_out()\n\nprint(\"\\nTerms in the corpus:\\n\", terms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:43:40.066456Z","iopub.execute_input":"2026-01-08T11:43:40.067012Z","iopub.status.idle":"2026-01-08T11:43:40.083649Z","shell.execute_reply.started":"2026-01-08T11:43:40.066985Z","shell.execute_reply":"2026-01-08T11:43:40.082751Z"}},"outputs":[{"name":"stdout","text":"\nTerms in the corpus:\n ['big' 'crocodile' 'green' 'scary' 'small']\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"**Step 4: Compute the semantic vector for the text \"green crocodile\"**","metadata":{}},{"cell_type":"code","source":"text = [\"green crocodile\"]\ntext_vector = vectorizer.transform(text)\nprint(\"\\nTF-IDF Vector for 'green crocodile':\\n\", text_vector.toarray())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:43:40.084680Z","iopub.execute_input":"2026-01-08T11:43:40.085504Z","iopub.status.idle":"2026-01-08T11:43:40.102468Z","shell.execute_reply.started":"2026-01-08T11:43:40.085476Z","shell.execute_reply":"2026-01-08T11:43:40.101562Z"}},"outputs":[{"name":"stdout","text":"\nTF-IDF Vector for 'green crocodile':\n [[0.         0.70710678 0.70710678 0.         0.        ]]\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"**Step 5: Compute cosine similarity between two documents (\"big crocodile” and “scary crocodile\")**","metadata":{}},{"cell_type":"code","source":"text_1 = [\"big crocodile\"]\ntext_vector_1 = vectorizer.transform(text_1)\n\ntext_2 = [\"scary crocodile\"]\ntext_vector_2 = vectorizer.transform(text_2)\n\ncosine_sim = cosine_similarity(text_vector_1, text_vector_2)\nprint(\"\\nCosine Similarity between 'big crocodile' and 'scary crocodile':\", cosine_sim[0][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:43:40.103773Z","iopub.execute_input":"2026-01-08T11:43:40.104099Z","iopub.status.idle":"2026-01-08T11:43:40.123313Z","shell.execute_reply.started":"2026-01-08T11:43:40.104063Z","shell.execute_reply":"2026-01-08T11:43:40.122567Z"}},"outputs":[{"name":"stdout","text":"\nCosine Similarity between 'big crocodile' and 'scary crocodile': 0.4280460350631186\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"# Using User defined functionality instead of using built in functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom math import log2, sqrt\n\n# Step 1: Document collection\ndocuments = {\n    \"d1\": \"scary green crocodile\",\n    \"d2\": \"scary green big\",\n    \"d3\": \"small crocodile\"\n}\n\n# Tokenize documents\ntokenized_docs = {k: v.split() for k, v in documents.items()}\n\n# Step 2: Vocabulary (alphabetical)\nvocab = sorted(set(word for doc in tokenized_docs.values() for word in doc))\nprint(\"Vocabulary:\", vocab)\n\n\n# Step 3: Term Frequency (TF)\ntf = pd.DataFrame(0, index=vocab, columns=documents.keys())\n\nfor doc, words in tokenized_docs.items():\n    for word in words:\n        tf.loc[word, doc] += 1\n\nprint(\"\\nTF Matrix:\")\nprint(tf)\n\n# Step 4: IDF computation\nN = len(documents)\nidf = {}\n\nfor term in vocab:\n    df = sum(term in doc for doc in tokenized_docs.values())\n    idf[term] = log2(N / df)\n\nidf_series = pd.Series(idf)\nprint(\"\\nIDF:\")\nprint(idf_series)\n\n# Step 5: TF-IDF Matrix\ntfidf = tf.multiply(idf_series, axis=0)\nprint(\"\\nTF-IDF Matrix:\")\nprint(tfidf)\n\n# Utility functions\ndef vector_length(vec):\n    return sqrt(sum(v**2 for v in vec))\n\ndef normalize(vec):\n    length = vector_length(vec)\n    return vec / length if length != 0 else vec\n\ndef cosine_similarity(v1, v2):\n    return np.dot(v1, v2) / (vector_length(v1) * vector_length(v2))\n\n\n# Question 1: Semantic Interpreter\n# \"green crocodile\"\nquery1 = [\"green\", \"crocodile\"]\nquery_vec = np.array([idf[word] if word in query1 else 0 for word in vocab])\n\nprint(\"\\nTF-IDF Vector for 'green crocodile':\")\nprint(query_vec)\n\nprint(\"\\nNormalized Vector:\")\nprint(normalize(query_vec))\n\n\n# Question 2: Similarity\n# \"big crocodile\" vs \"scary crocodile\"\nq1 = [\"big\", \"crocodile\"]\nq2 = [\"scary\", \"crocodile\"]\n\nvec1 = np.array([idf[word] if word in q1 else 0 for word in vocab])\nvec2 = np.array([idf[word] if word in q2 else 0 for word in vocab])\n\nprint(\"\\nVector for 'big crocodile':\", vec1)\nprint(\"Vector for 'scary crocodile':\", vec2)\n\nprint(\"\\nNormalized 'big crocodile':\", normalize(vec1))\nprint(\"Normalized 'scary crocodile':\", normalize(vec2))\n\nsimilarity = cosine_similarity(vec1, vec2)\nprint(\"\\nCosine Similarity:\", round(similarity, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T11:43:40.195239Z","iopub.execute_input":"2026-01-08T11:43:40.195530Z","iopub.status.idle":"2026-01-08T11:43:40.214698Z","shell.execute_reply.started":"2026-01-08T11:43:40.195506Z","shell.execute_reply":"2026-01-08T11:43:40.213863Z"}},"outputs":[{"name":"stdout","text":"Vocabulary: ['big', 'crocodile', 'green', 'scary', 'small']\n\nTF Matrix:\n           d1  d2  d3\nbig         0   1   0\ncrocodile   1   0   1\ngreen       1   1   0\nscary       1   1   0\nsmall       0   0   1\n\nIDF:\nbig          1.584963\ncrocodile    0.584963\ngreen        0.584963\nscary        0.584963\nsmall        1.584963\ndtype: float64\n\nTF-IDF Matrix:\n                 d1        d2        d3\nbig        0.000000  1.584963  0.000000\ncrocodile  0.584963  0.000000  0.584963\ngreen      0.584963  0.584963  0.000000\nscary      0.584963  0.584963  0.000000\nsmall      0.000000  0.000000  1.584963\n\nTF-IDF Vector for 'green crocodile':\n[0.        0.5849625 0.5849625 0.        0.       ]\n\nNormalized Vector:\n[0.         0.70710678 0.70710678 0.         0.        ]\n\nVector for 'big crocodile': [1.5849625 0.5849625 0.        0.        0.       ]\nVector for 'scary crocodile': [0.        0.5849625 0.        0.5849625 0.       ]\n\nNormalized 'big crocodile': [0.9381454  0.34624155 0.         0.         0.        ]\nNormalized 'scary crocodile': [0.         0.70710678 0.         0.70710678 0.        ]\n\nCosine Similarity: 0.245\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"# The answer are coming as different because of Normalization of the built-in functions.","metadata":{}}]}